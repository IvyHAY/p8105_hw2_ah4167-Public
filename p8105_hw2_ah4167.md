p8105_hw2_ah4167
================
Aiying Huang
2023-09-26

# problem 1

``` r
options(scipen=999)
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.3     ✔ readr     2.1.4
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.0
    ## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

First, clean the data in pols-month.csv. Use separate() to break up the
variable mon into integer variables year, month, and day; replace month
number with month name; create a president variable taking values gop
and dem, and remove prez_dem and prez_gop; and remove the day variable.

``` r
pols_month_df=
  read.csv("./data/fivethirtyeight_datasets/pols-month.csv")|>
  separate(
           col="mon",
           into=c("year","month","day"),
           sep="-")|>
  mutate(
    month=case_match(
      month,
    "01"~"Jan",
    "02"~"Feb",
    "03"~"Mar",
    "04"~"Apr",
    "05"~"May",
    "06"~"Jun",
    "07"~"Jul",
    "08"~"Aug",
    "09"~"Sep",
    "10"~"Oct",
    "11"~"Nov",
    "12"~"Dec")
  )|>
  mutate(
    president=case_match(
      prez_gop,
    0~"dem",
    1~"gop"
    )
  )|>
  mutate(year,year=as.numeric(year))|>
  select(-starts_with("prez_"))|>
  select(-day)
```

Second, clean the data in snp.csv using a similar process to the above.
For consistency across datasets, arrange according to year and month,
and organize so that year and month are the leading columns.

``` r
snp_df_1=
  read.csv("./data/fivethirtyeight_datasets/snp.csv")|>
  head(175)|>
  separate(
           col="date",
           into=c("year","month","day"),
           sep="/")|>
  mutate(
    year=case_match(
      year,
      "1"~2001,
    "2"~2002,
    "3"~2003,
    "4"~2004,
    "5"~2005,
    "6"~2006,
    "7"~2007,
    "8"~2008,
    "9"~2009,
    "10"~2010,
    "11"~2011,
    "12"~2012
    ),
    month=case_match(
      month,
    "1"~"Jan",
    "2"~"Feb",
    "3"~"Mar",
    "4"~"Apr",
    "5"~"May",
    "6"~"Jun",
    "7"~"Jul",
    "8"~"Aug",
    "9"~"Sep",
    "10"~"Oct",
    "11"~"Nov",
    "12"~"Dec")
  )|>
  select(-day)
```

``` r
snp_df_2=
  read.csv("./data/fivethirtyeight_datasets/snp.csv",skip=175)|>
  separate(
           col="X1.2.01",
           into=c("month","day","year"),
           sep="/")|>
  mutate(
    year=1900+as.numeric(year),
    month=case_match(
      month,
    "1"~"Jan",
    "2"~"Feb",
    "3"~"Mar",
    "4"~"Apr",
    "5"~"May",
    "6"~"Jun",
    "7"~"Jul",
    "8"~"Aug",
    "9"~"Sep",
    "10"~"Oct",
    "11"~"Nov",
    "12"~"Dec")
  )|>
  mutate(
    year=replace(year,year==1900,2000)
  )|>
  select(-day)|>
  rename(close=X1366.01001)
```

``` r
snp_df=
  bind_rows(snp_df_1,snp_df_2)|>
  arrange(year,month)
```

Third, tidy the unemployment data so that it can be merged with the
previous datasets. This process will involve switching from “wide” to
“long” format; ensuring that key variables have the same name; and
ensuring that key variables take the same values.

``` r
unemployment_df=
  read.csv("./data/fivethirtyeight_datasets/unemployment.csv")|>
  pivot_longer(
    Jan:Dec,
    names_to="month",
    values_to="unemployment"
  )|>
  janitor::clean_names()
```

Join the datasets by merging snp into pols, and merging unemployment
into the result.

``` r
Joint_df=
  left_join(pols_month_df,snp_df,by=c("year","month"))
final_merged_data <- left_join(Joint_df, unemployment_df, by = c("year","month"))
```

Write a short paragraph about these datasets. Explain briefly what each
dataset contained, and describe the resulting dataset (e.g. give the
dimension, range of years, and names of key variables).

------------------------------------------------------------------------

# Problem 2

Read and clean the Mr. Trash Wheel sheet:

- specify the sheet in the Excel file and to omit non-data entries (rows
  with notes / figures; columns containing notes) using arguments in
  read_excel
- use reasonable variable names
- omit rows that do not include dumpster-specific data

The data include a column for the (approximate) number of homes powered.
This calculation is described in the Homes powered note, but not applied
to every row in the dataset. Update the data to include a new
homes_powered variable based on this calculation.

``` r
Mr_Trash_Wheel_df=
  readxl::read_excel("./data/202207 Trash Wheel Collection Data.xlsx",sheet="Mr. Trash Wheel",range="A2:N549")|>
  janitor::clean_names()|>
  mutate(homes_powered=weight_tons*500/30)|>
  mutate(wheel_type="Mr")|>
  mutate(year,year=as.numeric(year))
```

- Use a similar process to import, clean, and organize the data for
  Professor Trash Wheel and Gwynnda, and combine these with the
  Mr. Trash Wheel dataset to produce a single tidy dataset. To keep
  track of which Trash Wheel is which, you may need to add an additional
  variable to all datasets before combining.

``` r
Professor_Trash_Wheel_df=
  readxl::read_excel("./data/202207 Trash Wheel Collection Data.xlsx",sheet="Professor Trash Wheel",range="A2:M96")|>
  janitor::clean_names()|>
  mutate(homes_powered=weight_tons*500/30)|>
  mutate(wheel_type="Professor")

Gwynnda_Trash_Wheel_df=
  readxl::read_excel("./data/202207 Trash Wheel Collection Data.xlsx",sheet="Gwynnda Trash Wheel",range="A2:K108")|>
  janitor::clean_names()|>
  mutate(homes_powered=weight_tons*500/30)|>
  mutate(wheel_type="Gwynnda")
```

``` r
Trash_Wheel_df=
  bind_rows(Mr_Trash_Wheel_df,
            Professor_Trash_Wheel_df,
            Gwynnda_Trash_Wheel_df)|>
  relocate(wheel_type)
```

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables. For available data, what was the total
weight of trash collected by Professor Trash Wheel? What was the total
number of cigarette butts collected by Gwynnda in July of 2021?

The number of observations in the resulting dataset is 747. There are 16
variables. We can tell from the resulting dataset that from 2014 to 2022
, the number of homes powered by trash recorded by this survey is about
37488. The total weight of trash collected by Professor Trash Wheel is
190.12. The the total number of cigarette butts collected by Gwynnda in
July of 2021 is 16300. All of those suggest that this project is
meaningful and it really did a lot in trash collecting and making it
useful to the public.

# Problem 3

Import, clean, and tidy the dataset of baseline demographics. Ensure
that sex and APOE4 carrier status are appropriate encoded (i.e. not
numeric), and remove any participants who do not meet the stated
inclusion criteria (i.e. no MCI at baseline).

``` r
baseline_df=read.csv("./data/data_mci/MCI_baseline.csv",skip=1)|>
  janitor::clean_names()|>
  mutate(
    sex=case_match(
      sex,
      0~"Male",
      1~"Female"
    ))|>
  mutate(
    apoe4=case_match(
      apoe4,
      0~"non-carrier",
      1~"APOE4 carrier"
    )
  )|>
  subset(
    age_at_onset!="."
  )
```

Discuss important steps in the import process and relevant features of
the dataset. How many participants were recruited, and of these how many
develop MCI? What is the average baseline age? What proportion of women
in the study are APOE4 carriers?

# Discuss important steps in the import process needed to be added.

There are 483 participants recruited, 97 of these develop MCI. The
average baseline age is about 66. The proprtion of women in the study
are APOE4 carriers is 0.6078431.

Similarly, import, clean, and tidy the dataset of longitudinally
observed biomarker values; comment on the steps on the import process
and the features of the dataset.

``` r
mci_amyloid_df=read.csv("./data/data_mci/mci_amyloid.csv",skip=1)|>
  janitor::clean_names()|>
  subset(
   baseline!="NA"
  )|>
  pivot_longer(
    baseline:time_8,
    names_to="time",
    values_to="amyloid_ratio"
  )
```

I firstly import the data and omit non-data entries. Then I remove any
participants who have no MCI at baseline. Finally I transfer the wide
format data to long format data. There are There are 487 participants
recruited, 486 of them have the biomarker at the baseline. The
proportion of those participants who complete the whole process is
0.7139918.
